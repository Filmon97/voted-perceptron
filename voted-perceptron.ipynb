{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "KwNNxpBEpv9c",
    "outputId": "aca67217-cda8-4cbd-8e82-4685136f0347"
   },
   "outputs": [],
   "source": [
    "# install the last version of numba\n",
    "pip install --upgrade numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vUIgFw55jQt"
   },
   "source": [
    "This notebook need the High Memory mode of Google Colab.\n",
    "\n",
    "To do this we will ran out the ram and then a popout will ask you to get more ram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRdvjA7Xpv9S"
   },
   "outputs": [],
   "source": [
    "# this is for upgrading RAM on google colab\n",
    "a = []\n",
    "while(1):\n",
    "    a.append('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMl-7Ypv6rj8"
   },
   "source": [
    "Now execute the full script, this script contains all the code hosted on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psjpuNRfpv9X"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "full_script.py\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "from math import copysign\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import faulthandler\n",
    "\n",
    "faulthandler.enable()\n",
    "\n",
    "\n",
    "@njit\n",
    "def train(X, y, epochs, kernel_degree):\n",
    "    \"\"\"Train the voted perceptron.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            X : ndarray\n",
    "                An ndarray where each row is a training case of the mnist dataset.\n",
    "\n",
    "            y : ndarray\n",
    "                An ndarray where each element is the label/classification of a\n",
    "                training case in train_set for binary classification.\n",
    "                Valid label values are -1 and 1.\n",
    "\n",
    "            epochs : int\n",
    "                The number of epochs to train.\n",
    "\n",
    "            kernel_degree: int\n",
    "                The number of the degree in the polynomial kernel\n",
    "            \"\"\"\n",
    "\n",
    "    # prediction vector's xi\n",
    "    # v_train_indices = []\n",
    "    # prediction vector's labels\n",
    "    # v_label_coeffs = []\n",
    "    # weights of the prediction vectors\n",
    "\n",
    "    # v1 = np.zeros(X.shape[1])\n",
    "    # don't call np.array on a np.array var\n",
    "    v_train_indices = np.array([0], dtype=np.int64)\n",
    "    v_label_coeffs = np.array([0], dtype=np.int64)\n",
    "    c = np.array([0], dtype=np.int64)\n",
    "    # they all have value = 0\n",
    "    # i will not consider the first elements\n",
    "    weight = 0\n",
    "    mistakes = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # for xi, label in zip(X, y):\n",
    "        # numba don't support nested arrays\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            xi = X[i]\n",
    "            label = y[i]\n",
    "            # same here i can't use sum over the prediction vector\n",
    "            # we need to iterate over a variable\n",
    "            # we define a new function\n",
    "            y_hat = copysign(1, implicit_form_product(\n",
    "                X, v_train_indices, v_label_coeffs, xi, kernel_degree)[-1])\n",
    "            # we take always the last prediction vector's product\n",
    "            # complexity of implicit_form_product is O(k)\n",
    "            if y_hat == label:\n",
    "                weight = weight + 1\n",
    "            else:\n",
    "                c = np.append(c, np.array([weight]), axis=0)\n",
    "                v_train_indices = np.append(\n",
    "                    v_train_indices, np.array([i]), axis=0)\n",
    "                v_label_coeffs = np.append(\n",
    "                    v_label_coeffs, np.array([label]), axis=0)\n",
    "                # reset #C_k+1 = 1\n",
    "                weight = 1\n",
    "                mistakes = mistakes + 1\n",
    "\n",
    "    c = np.append(c, np.array([weight]), axis=0)\n",
    "    c = c[1:c.shape[0]]\n",
    "    return v_train_indices, v_label_coeffs, c, mistakes\n",
    "\n",
    "\n",
    "@njit  # (parallel=True)\n",
    "def implicit_form_product(X, v_train_indices, v_label_coeffs, x, kernel_degree):\n",
    "    v_x = np.empty(v_train_indices.shape[0], dtype=np.float32)\n",
    "    # the first dot_product is y0 = 1 *polynomial_expansion(x0 = 0_vect,x)\n",
    "    v_x[0] = polynomial_expansion(np.zeros(X.shape[1], dtype=np.float32), x, kernel_degree)\n",
    "    for k in range(1, v_train_indices.shape[0]):\n",
    "        xi = X[v_train_indices[k]]\n",
    "        yi = v_label_coeffs[k]\n",
    "        v_x[k] = v_x[k - 1] + yi * polynomial_expansion(xi, x, kernel_degree)\n",
    "\n",
    "    return v_x\n",
    "\n",
    "\n",
    "@njit  # (parallel=True)\n",
    "def implicit_form_v(X, v_train_indices, v_label_coeffs):\n",
    "    v = np.empty(v_train_indices.shape[0], dtype=np.float32)\n",
    "    # v0\n",
    "    v[0] = 0\n",
    "    # the first product is y0 = 1 * x0 = 0_vect\n",
    "    for k in range(1, v_train_indices.shape[0]):\n",
    "        yi = v_label_coeffs[k]\n",
    "        xi = X[v_train_indices[k]]\n",
    "        v[k] = v[k - 1] + (yi * xi)\n",
    "\n",
    "    return v\n",
    "\n",
    "\n",
    "@njit\n",
    "def polynomial_expansion(xi, xj, d):\n",
    "    return (1 + np.dot(xi, xj)) ** d\n",
    "\n",
    "\n",
    "# _________________________________________________________________________________\n",
    "# prediction functions\n",
    "\n",
    "\n",
    "@njit\n",
    "def last_unnormalized(X, v_train_indices, v_label_coeffs, x, kernel_degree):\n",
    "    \"\"\"Compute score using the final prediction vector(unnormalized)\"\"\"\n",
    "    \"\"\" x: unlabeled instance\"\"\"\n",
    "    score = implicit_form_product(X,\n",
    "                                  v_train_indices, v_label_coeffs, x, kernel_degree)[-1]\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "@njit\n",
    "def normalize(score, v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0:\n",
    "        return score\n",
    "    return score / norm\n",
    "\n",
    "\n",
    "@njit\n",
    "def last_normalized(X, v_train_indices, v_label_coeffs, x, kernel_degree):\n",
    "    \"\"\"Compute score using the final prediction vector(normalized)\"\"\"\n",
    "    \"\"\" x: unlabeled instance\"\"\"\n",
    "    score = last_unnormalized(\n",
    "        X, v_train_indices, v_label_coeffs, x, kernel_degree)\n",
    "\n",
    "    return normalize(score, implicit_form_v(X, v_train_indices, v_label_coeffs)[-1])\n",
    "\n",
    "\n",
    "@njit\n",
    "def vote(X, v_train_indices, v_label_coeffs, c, x, kernel_degree):\n",
    "    \"\"\"Compute score using analog of the deterministic leave-one-out conversion\"\"\"\n",
    "    \"\"\" x: unlabeled instance\"\"\"\n",
    "\n",
    "    dot_products = implicit_form_product(X,\n",
    "                                         v_train_indices, v_label_coeffs, x, kernel_degree)\n",
    "\n",
    "    s = np.empty(v_train_indices.shape[0])\n",
    "    s[0] = 0\n",
    "    for i in range(1, v_train_indices.shape[0]):\n",
    "        weight = c[i]\n",
    "        v_x = dot_products[i]\n",
    "        s[i] = weight * copysign(1, v_x)\n",
    "\n",
    "    return np.sum(s)\n",
    "\n",
    "\n",
    "@njit\n",
    "def avg_unnormalized(X, v_train_indices, v_label_coeffs, c, x, kernel_degree):\n",
    "    \"\"\"Compute score using an average of the prediction vectors\"\"\"\n",
    "    \"\"\" x: unlabeled instance\"\"\"\n",
    "\n",
    "    dot_products = implicit_form_product(X,\n",
    "                                         v_train_indices, v_label_coeffs, x, kernel_degree)\n",
    "\n",
    "    s = np.empty(v_train_indices.shape[0])\n",
    "    s[0] = 0\n",
    "    for i in range(1, v_train_indices.shape[0]):\n",
    "        weight = c[i]\n",
    "        v_x = dot_products[i]\n",
    "        s[i] = weight * v_x\n",
    "\n",
    "    return np.sum(s)\n",
    "\n",
    "\n",
    "@njit\n",
    "def avg_normalized(X, v_train_indices, v_label_coeffs, c, x, kernel_degree):\n",
    "    \"\"\"Compute score using an average of the prediction vectors(normalized)\"\"\"\n",
    "    \"\"\" x: unlabeled instance\"\"\"\n",
    "\n",
    "    dot_products = implicit_form_product(X,\n",
    "                                         v_train_indices, v_label_coeffs, x, kernel_degree)\n",
    "    v = implicit_form_v(X, v_train_indices, v_label_coeffs)\n",
    "    s = np.empty(v_train_indices.shape[0])\n",
    "    s[0] = 0\n",
    "    for i in range(1, v_train_indices.shape[0]):\n",
    "        weight = c[i]\n",
    "        v_x = dot_products[i]\n",
    "        s[i] = weight * normalize(v_x, v[i])\n",
    "\n",
    "    return np.sum(s)\n",
    "\n",
    "\n",
    "@njit\n",
    "def random_unnormalized(X, v_train_indices, v_label_coeffs, c, x, kernel_degree):\n",
    "    \"\"\"Compute score using analog of the randomized leave-one-out \n",
    "    method in which we predict using the prediction vectors \n",
    "    that exist at a randomly chosen “time slice.”\"\"\"\n",
    "    \"\"\" x: unlabeled instance\"\"\"\n",
    "    t = np.sum(c)\n",
    "    # time slice\n",
    "    r = np.random.randint(t + 1)\n",
    "    rl_sum = 0\n",
    "    rl = 1\n",
    "    for i in range(1, c.shape[0]):\n",
    "        if rl_sum > r:\n",
    "            break\n",
    "        rl_sum = rl_sum + c[i]\n",
    "        rl = rl + 1\n",
    "    rl = rl - 1\n",
    "    score = implicit_form_product(\n",
    "        X, v_train_indices, v_label_coeffs, x, kernel_degree)[rl]\n",
    "    return score\n",
    "\n",
    "\n",
    "@njit\n",
    "def random_normalized(X, v_train_indices, v_label_coeffs, c, x, kernel_degree):\n",
    "    \"\"\"Compute score using analog of the randomized leave-one-out \n",
    "    method in which we predict using the prediction vectors \n",
    "    that exist at a randomly chosen “time slice.(normalized)”\"\"\"\n",
    "    \"\"\" x: unlabeled instance\"\"\"\n",
    "    t = np.sum(c)\n",
    "    # time slice\n",
    "    # np.random.random_integers(low=0, high=t)  inclusive(low and high)\n",
    "    # numba doesn't support random_integers\n",
    "    # randint is exclusive\n",
    "    r = np.random.randint(t + 1)\n",
    "\n",
    "    score = implicit_form_product(\n",
    "        X, v_train_indices, v_label_coeffs, x, kernel_degree)\n",
    "\n",
    "    rl_sum = 0\n",
    "    rl = 1\n",
    "    for i in range(1, c.shape[0]):\n",
    "        if rl_sum > r:\n",
    "            break\n",
    "        rl_sum = rl_sum + c[i]\n",
    "        rl = rl + 1\n",
    "    rl = rl - 1\n",
    "    score = implicit_form_product(\n",
    "        X, v_train_indices, v_label_coeffs, x, kernel_degree)[rl]\n",
    "\n",
    "    return normalize(score, implicit_form_v(X, v_train_indices, v_label_coeffs)[rl])\n",
    "\n",
    "\n",
    "@njit\n",
    "def highest_score_arg(s):\n",
    "    return np.argmax(s)\n",
    "\n",
    "\n",
    "@njit\n",
    "def highest_score(s):\n",
    "    return np.max(s)\n",
    "\n",
    "\n",
    "# _________________________________________________________________________________\n",
    "# model functions\n",
    "\n",
    "# not numba\n",
    "def fit(X, y, epoch, kernel_degree):\n",
    "    return Parallel(n_jobs=4, prefer=\"threads\")(delayed(model)(X, y, i, epoch, kernel_degree) for i in range(10))\n",
    "\n",
    "\n",
    "@njit\n",
    "def model(X, y, class_type, epoch, kernel_degree):\n",
    "    y = np.where(y == class_type, 1, -1)\n",
    "    if epoch < 1:\n",
    "            # contiguous arrays\n",
    "        fraction_x = X[0:int(X.shape[0] * epoch),\n",
    "                       :].copy()\n",
    "        fraction_y = y[0:int(X.shape[0] * epoch)].copy()\n",
    "        return train(fraction_x, fraction_y, 1, kernel_degree)\n",
    "    return train(X, y, epoch, kernel_degree)\n",
    "\n",
    "\n",
    "@njit\n",
    "def predictions(X, v_train_indices, v_label_coeffs, c, x, kernel_degree):\n",
    "    s_random = random_unnormalized(\n",
    "        X, v_train_indices, v_label_coeffs, c, x, kernel_degree)\n",
    "    s_last = last_unnormalized(\n",
    "        X, v_train_indices, v_label_coeffs, x, kernel_degree)\n",
    "    s_avg = avg_unnormalized(\n",
    "        X, v_train_indices, v_label_coeffs, c, x, kernel_degree)\n",
    "    s_vote = vote(X, v_train_indices, v_label_coeffs, c, x, kernel_degree)\n",
    "\n",
    "    return np.array([s_random, s_last, s_avg, s_vote])\n",
    "\n",
    "\n",
    "# _________________________________________________________________________________\n",
    "# Kernel Matrix version (Gram)\n",
    "\n",
    "# for the kernel matrix\n",
    "@njit\n",
    "def gram_build(X,Y, kernel_degree):\n",
    "    Gram = np.zeros((X.shape[0], Y.shape[0]), dtype=np.float32)\n",
    "    for i in range(Gram.shape[0]):\n",
    "        for j in range(i, Gram.shape[0]):\n",
    "            if i <= j:\n",
    "                Gram[i, j] = polynomial_expansion(X[i], Y[j], kernel_degree)\n",
    "                Gram[j, i] = Gram[i, j]\n",
    "    return Gram\n",
    "\n",
    "\n",
    "def gram_fit(X, y, epoch, kernel_degree):\n",
    "    return Parallel(n_jobs=2, prefer=\"threads\")(delayed(gram_model)(X, y, i, epoch, kernel_degree) for i in range(10))\n",
    "\n",
    "\n",
    "@njit\n",
    "def gram_model(X, y, class_type, epoch, kernel_degree):\n",
    "    y = np.where(y == class_type, 1, -1)\n",
    "    if epoch < 1:\n",
    "            # contiguous arrays\n",
    "        fraction_x = X[0:int(X.shape[0] * epoch),\n",
    "                       :].copy()\n",
    "        fraction_y = y[0:int(X.shape[0] * epoch)].copy()\n",
    "        return gram_train(fraction_x, fraction_y, 1, kernel_degree)\n",
    "    return gram_train(X, y, epoch, kernel_degree)\n",
    "\n",
    "\n",
    "@njit\n",
    "def gram_predictions(X, v_train_indices, v_label_coeffs, c, x, kernel_degree, gram_index):\n",
    "    s_random = gram_random_unnormalized(\n",
    "        X, v_train_indices, v_label_coeffs, c, x, kernel_degree, gram_index)\n",
    "    s_last = gram_last_unnormalized(\n",
    "        X, v_train_indices, v_label_coeffs, x, kernel_degree, gram_index)\n",
    "    s_avg = gram_avg_unnormalized(\n",
    "        X, v_train_indices, v_label_coeffs, c, x, kernel_degree, gram_index)\n",
    "    s_vote = gram_vote(X, v_train_indices, v_label_coeffs,\n",
    "                       c, x, kernel_degree, gram_index)\n",
    "\n",
    "    return np.array([s_random, s_last, s_avg, s_vote])\n",
    "\n",
    "\n",
    "@njit\n",
    "def gram_train(X, y, epochs, kernel_degree):\n",
    "    v_train_indices = np.array([0], dtype=np.int64)\n",
    "    v_label_coeffs = np.array([0], dtype=np.int64)\n",
    "    c = np.array([0], dtype=np.int64)\n",
    "    weight = 0\n",
    "    mistakes = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            xi = X[i]\n",
    "            label = y[i]\n",
    "\n",
    "            y_hat = copysign(1, gram_implicit_form_product(\n",
    "                X, v_train_indices, v_label_coeffs, xi, kernel_degree, i)[-1])\n",
    "            if y_hat == label:\n",
    "                weight = weight + 1\n",
    "            else:\n",
    "                c = np.append(c, np.array([weight]), axis=0)\n",
    "                v_train_indices = np.append(\n",
    "                    v_train_indices, np.array([i]), axis=0)\n",
    "                v_label_coeffs = np.append(\n",
    "                    v_label_coeffs, np.array([label]), axis=0)\n",
    "                weight = 1\n",
    "                mistakes = mistakes + 1\n",
    "\n",
    "    c = np.append(c, np.array([weight]), axis=0)\n",
    "    c = c[1:c.shape[0]]\n",
    "    return v_train_indices, v_label_coeffs, c, mistakes\n",
    "\n",
    "\n",
    "@njit\n",
    "def gram_implicit_form_product(X, v_train_indices, v_label_coeffs, x, kernel_degree, gram_index):\n",
    "    v_x = np.empty(v_train_indices.shape[0], dtype=np.float32)\n",
    "    v_x[0] = polynomial_expansion(\n",
    "        np.zeros(X.shape[1], dtype=np.float32), x, kernel_degree)\n",
    "    for k in range(1, v_train_indices.shape[0]):\n",
    "        yi = v_label_coeffs[k]\n",
    "        v_x[k] = v_x[k - 1] + yi * Gram_train[gram_index, v_train_indices[k]]\n",
    "\n",
    "    return v_x\n",
    "\n",
    "\n",
    "@njit\n",
    "def gram_test_implicit_form_product(X, v_train_indices, v_label_coeffs, x, kernel_degree, gram_index):\n",
    "    v_x = np.empty(v_train_indices.shape[0], dtype=np.float32)\n",
    "    v_x[0] = polynomial_expansion(\n",
    "        np.zeros(X.shape[1], dtype=np.float32), x, kernel_degree)\n",
    "    for k in range(1, v_train_indices.shape[0]):\n",
    "        yi = v_label_coeffs[k]\n",
    "        v_x[k] = v_x[k - 1] + yi * Gram_test[gram_index, v_train_indices[k]]\n",
    "\n",
    "    return v_x\n",
    "\n",
    "\n",
    "@njit\n",
    "def gram_last_unnormalized(X, v_train_indices, v_label_coeffs, x, kernel_degree, gram_index):\n",
    "    \"\"\"Compute score using the final prediction vector(unnormalized)\"\"\"\n",
    "    \"\"\" x: unlabeled instance\"\"\"\n",
    "    score = gram_test_implicit_form_product(X,\n",
    "                                            v_train_indices, v_label_coeffs, x, kernel_degree, gram_index)[-1]\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "@njit\n",
    "def gram_vote(X, v_train_indices, v_label_coeffs, c, x, kernel_degree, gram_index):\n",
    "    \"\"\"Compute score using analog of the deterministic leave-one-out conversion\"\"\"\n",
    "    \"\"\" x: unlabeled instance\"\"\"\n",
    "\n",
    "    dot_products = gram_test_implicit_form_product(X,\n",
    "                                                   v_train_indices, v_label_coeffs, x, kernel_degree, gram_index)\n",
    "\n",
    "    s = np.empty(v_train_indices.shape[0])\n",
    "    s[0] = 0\n",
    "    for i in range(1, v_train_indices.shape[0]):\n",
    "        weight = c[i]\n",
    "        v_x = dot_products[i]\n",
    "        s[i] = weight * copysign(1, v_x)\n",
    "\n",
    "    return np.sum(s)\n",
    "\n",
    "\n",
    "@njit\n",
    "def gram_avg_unnormalized(X, v_train_indices, v_label_coeffs, c, x, kernel_degree, gram_index):\n",
    "    \"\"\"Compute score using an average of the prediction vectors\"\"\"\n",
    "    \"\"\" x: unlabeled instance\"\"\"\n",
    "\n",
    "    dot_products = gram_test_implicit_form_product(X,\n",
    "                                                   v_train_indices, v_label_coeffs, x, kernel_degree, gram_index)\n",
    "\n",
    "    s = np.empty(v_train_indices.shape[0])\n",
    "    s[0] = 0\n",
    "    for i in range(1, v_train_indices.shape[0]):\n",
    "        weight = c[i]\n",
    "        v_x = dot_products[i]\n",
    "        s[i] = weight * v_x\n",
    "\n",
    "    return np.sum(s)\n",
    "\n",
    "\n",
    "@njit\n",
    "def gram_random_unnormalized(X, v_train_indices, v_label_coeffs, c, x, kernel_degree, gram_index):\n",
    "    \"\"\"Compute score using analog of the randomized leave-one-out \n",
    "    method in which we predict using the prediction vectors \n",
    "    that exist at a randomly chosen “time slice.”\"\"\"\n",
    "    \"\"\" x: unlabeled instance\"\"\"\n",
    "    t = np.sum(c)\n",
    "    # time slice\n",
    "    r = np.random.randint(t + 1)\n",
    "    rl_sum = 0\n",
    "    rl = 1\n",
    "    for i in range(1, c.shape[0]):\n",
    "        if rl_sum > r:\n",
    "            break\n",
    "        rl_sum = rl_sum + c[i]\n",
    "        rl = rl + 1\n",
    "    rl = rl - 1\n",
    "    score = gram_test_implicit_form_product(\n",
    "        X, v_train_indices, v_label_coeffs, x, kernel_degree, gram_index)[rl]\n",
    "    return score\n",
    "\"\"\"\n",
    "full_script utils\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import urllib.request\n",
    "from urllib.parse import urljoin\n",
    "import gzip\n",
    "\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "# _________________________________________________________________________________\n",
    "# Progress Bar\n",
    "\n",
    "\n",
    "class MnistDataset:\n",
    "    \"\"\"Mnist utils\"\"\"\n",
    "\n",
    "    def __init__(self, refresh=False):\n",
    "        self.mnist_path_dir = 'mnist'\n",
    "        self.datasets_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "        self.refresh = refresh\n",
    "\n",
    "    def download_file(self, download_file):\n",
    "\n",
    "        output_file = os.path.join(self.mnist_path_dir, download_file)\n",
    "\n",
    "        if self.refresh or not os.path.isfile(output_file):\n",
    "            print('downloading {0} from {1}'.format(\n",
    "                download_file, self.datasets_url))\n",
    "            url = urljoin(self.datasets_url, download_file)\n",
    "            download_url(url, output_file)\n",
    "\n",
    "        return output_file\n",
    "\n",
    "    def load_mnist(self, train_test='train'):\n",
    "\n",
    "        try:\n",
    "            os.makedirs(self.mnist_path_dir, exist_ok=False)\n",
    "            print('Creating mnist directory')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Load MNIST dataset from 'path'\n",
    "        labels_path = self.download_file(\n",
    "            '{}-labels-idx1-ubyte.gz'.format(train_test))\n",
    "        images_path = self.download_file(\n",
    "            '{}-images-idx3-ubyte.gz'.format(train_test))\n",
    "\n",
    "        with gzip.open(labels_path, 'rb') as lbpath:\n",
    "            lbpath.read(8)\n",
    "            buffer = lbpath.read()\n",
    "            labels = np.frombuffer(buffer, dtype=np.uint8)\n",
    "\n",
    "        with gzip.open(images_path, 'rb') as imgpath:\n",
    "            imgpath.read(16)\n",
    "            buffer = imgpath.read()\n",
    "            images = np.frombuffer(buffer,\n",
    "                                   dtype=np.uint8).reshape(len(labels),\n",
    "                                                           784).astype(np.float32)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def train_dataset(self):\n",
    "        return self.load_mnist(train_test='train')\n",
    "\n",
    "    def test_dataset(self):\n",
    "        return self.load_mnist(train_test='t10k')\n",
    "\n",
    "# _________________________________________________________________________________\n",
    "# Progress Bar\n",
    "\n",
    "\n",
    "class ProgressBar(tqdm):\n",
    "    \"\"\"Progress utils\"\"\"\n",
    "\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def download_url(url, output_file):\n",
    "    with ProgressBar(unit='B', unit_scale=True,\n",
    "                     miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(\n",
    "            url, filename=output_file, reporthook=t.update_to)\n",
    "\n",
    "# _________________________________________________________________________________\n",
    "# model save & load\n",
    "\n",
    "\n",
    "class Pretrained:\n",
    "    \"\"\"Pretrained utils\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_path_dir = 'models'\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        try:\n",
    "            os.makedirs(self.model_path_dir, exist_ok=False)\n",
    "            #print('Creating models directory')\n",
    "        except:\n",
    "            pass\n",
    "        i = 0\n",
    "        output_file = os.path.join(\n",
    "            self.model_path_dir, filename) + '_{}.pkl'.format(i)\n",
    "        while os.path.isfile(output_file):\n",
    "            i = i + 1\n",
    "            output_file = os.path.join(\n",
    "                self.model_path_dir, filename) + '_{}.pkl'.format(i)\n",
    "        # compression level = 9\n",
    "        joblib.dump(model, output_file, 9)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        input_file = os.path.join(self.model_path_dir, filename) + '.pkl'\n",
    "        return joblib.load(input_file)\n",
    "\n",
    "# _________________________________________________________________________________\n",
    "# Experiment utils\n",
    "\n",
    "\n",
    "def test_error(X, models, test, label, kernel_degree):\n",
    "    scores_random = np.empty(test.shape[0])\n",
    "    scores_last = np.empty(test.shape[0])\n",
    "    scores_avg = np.empty(test.shape[0])\n",
    "    scores_vote = np.empty(test.shape[0])\n",
    "    j = 0\n",
    "    for x in test:\n",
    "        s_random = np.empty(10)\n",
    "        s_last = np.empty(10)\n",
    "        s_avg = np.empty(10)\n",
    "        s_vote = np.empty(10)\n",
    "        for i in range(10):\n",
    "            predictions_array = predictions(\n",
    "                X, models[i, 0], models[i, 1], models[i, 2], x, kernel_degree)\n",
    "            s_random[i] = predictions_array[0]\n",
    "            s_last[i] = predictions_array[1]\n",
    "            s_avg[i] = predictions_array[2]\n",
    "            s_vote[i] = predictions_array[3]\n",
    "        # Survival Of The Fittest\n",
    "        scores_random[j] = highest_score_arg(s_random)\n",
    "        scores_last[j] = highest_score_arg(s_last)\n",
    "        scores_avg[j] = highest_score_arg(s_avg)\n",
    "        scores_vote[j] = highest_score_arg(s_vote)\n",
    "        j = j + 1\n",
    "\n",
    "    error_random = np.sum(scores_random != label) / label.shape[0]\n",
    "    error_last = np.sum(scores_last != label) / label.shape[0]\n",
    "    error_avg = np.sum(scores_avg != label) / label.shape[0]\n",
    "    error_vote = np.sum(scores_vote != label) / label.shape[0]\n",
    "\n",
    "    return error_random, error_last, error_avg, error_vote\n",
    "\n",
    "\n",
    "def n_mistakes(models):\n",
    "    m = 0\n",
    "    for o in range(10):\n",
    "        m = m + models[o, 3]\n",
    "    return m\n",
    "\n",
    "\n",
    "def n_supvect(models):\n",
    "    s_v = 0\n",
    "    for o in range(10):\n",
    "        s_v = s_v + models[o, 1].shape[0]\n",
    "    return s_v\n",
    "\n",
    "\n",
    "def save_models(models, epoch, kernel_degree):\n",
    "    # print(\"saving models in models/...\")\n",
    "    pretrained = Pretrained()\n",
    "    if epoch < 1:\n",
    "        epoch = '0_{}'.format(int(epoch * 10))\n",
    "    pretrained.save_model(\n",
    "        models, 'pretrained_e{0}_k{1}'.format(epoch, kernel_degree))\n",
    "\n",
    "\n",
    "def load_models(epoch, kernel_degree, same):\n",
    "    # print(\"loading models from models/...\")\n",
    "    pretrained = Pretrained()\n",
    "    if epoch < 1:\n",
    "        epoch = '0_{}'.format(int(epoch * 10))\n",
    "    return pretrained.load_model('pretrained_e{0}_k{1}_{2}'.format(epoch, kernel_degree, same))\n",
    "\n",
    "\n",
    "def train_and_store(X_train, y_train, epoch, kernel_degree):\n",
    "    models = np.array(fit(X_train, y_train, epoch, kernel_degree))\n",
    "    save_models(models, epoch, kernel_degree)\n",
    "\n",
    "def gram_train_and_store(X_train, y_train, epoch, kernel_degree):\n",
    "    models = np.array(gram_fit(X_train, y_train, epoch, kernel_degree))\n",
    "    save_models(models, epoch, kernel_degree)\n",
    "\n",
    "def load_and_test(X_train, X_test, y_test, epoch, kernel_degree, same=0):\n",
    "    models = load_models(epoch, kernel_degree, same)\n",
    "    e_r, e_l, e_a, e_v = test_error(\n",
    "        X_train, models, X_test, y_test, kernel_degree)\n",
    "    perc_r = e_r * 100\n",
    "    perc_l = e_l * 100\n",
    "    perc_a = e_a * 100\n",
    "    perc_v = e_v * 100\n",
    "    # print(\"{0:.2f}\".format(perc))\n",
    "    return perc_r, perc_l, perc_a, perc_v\n",
    "\n",
    "def gram_load_and_test(X_train, X_test, y_test, epoch, kernel_degree, same=0):\n",
    "    models = load_models(epoch, kernel_degree, same)\n",
    "    e_r, e_l, e_a, e_v = gram_test_error(\n",
    "        X_train, models, X_test, y_test, kernel_degree)\n",
    "    perc_r = e_r * 100\n",
    "    perc_l = e_l * 100\n",
    "    perc_a = e_a * 100\n",
    "    perc_v = e_v * 100\n",
    "    return perc_r, perc_l, perc_a, perc_v\n",
    "\n",
    "# gram test error\n",
    "def gram_test_error(X, models, test, label, kernel_degree):\n",
    "    scores_random = np.empty(test.shape[0])\n",
    "    scores_last = np.empty(test.shape[0])\n",
    "    scores_avg = np.empty(test.shape[0])\n",
    "    scores_vote = np.empty(test.shape[0])\n",
    "    j = 0\n",
    "    for t in range(test.shape[0]):\n",
    "        x = test[t]\n",
    "        s_random = np.empty(10)\n",
    "        s_last = np.empty(10)\n",
    "        s_avg = np.empty(10)\n",
    "        s_vote = np.empty(10)\n",
    "        for i in range(10):\n",
    "            predictions_array = gram_predictions(\n",
    "                X, models[i, 0], models[i, 1], models[i, 2], x, kernel_degree, t)\n",
    "            s_random[i] = predictions_array[0]\n",
    "            s_last[i] = predictions_array[1]\n",
    "            s_avg[i] = predictions_array[2]\n",
    "            s_vote[i] = predictions_array[3]\n",
    "        # Survival Of The Fittest\n",
    "        scores_random[j] = highest_score_arg(s_random)\n",
    "        scores_last[j] = highest_score_arg(s_last)\n",
    "        scores_avg[j] = highest_score_arg(s_avg)\n",
    "        scores_vote[j] = highest_score_arg(s_vote)\n",
    "        j = j + 1\n",
    "\n",
    "    error_random = np.sum(scores_random != label) / label.shape[0]\n",
    "    error_last = np.sum(scores_last != label) / label.shape[0]\n",
    "    error_avg = np.sum(scores_avg != label) / label.shape[0]\n",
    "    error_vote = np.sum(scores_vote != label) / label.shape[0]\n",
    "\n",
    "    return error_random, error_last, error_avg, error_vote\n",
    "# plot function\n",
    "\n",
    "\n",
    "def simple_plot(x, error_random, error_last, error_avg, error_vote, kernel_degree):\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x, error_random, label='random(unorm)')\n",
    "    ax.plot(x, error_last, label='last(unorm)')\n",
    "    ax.plot(x, error_avg, label='avg(unorm)')\n",
    "    ax.plot(x, error_vote, label='vote')\n",
    "    ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "    ax.set_title('d={}'.format(kernel_degree))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Test Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def log_plot(x, error_random, error_last, error_avg, error_vote, kernel_degree):\n",
    "    \"\"\" errors should contains:\n",
    "        - error_random,\n",
    "        - error_last,\n",
    "        - error_avg,\n",
    "        - error_vote\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn')\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.semilogx(x, error_random, label='random(unorm)')\n",
    "    ax.semilogx(x, error_last, label='last(unorm)')\n",
    "    ax.semilogx(x, error_avg, label='avg(unorm)')\n",
    "    ax.semilogx(x, error_vote, label='vote')\n",
    "    ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "    ax.set_title('d={}'.format(kernel_degree))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Test Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFFApqjqpv9e"
   },
   "source": [
    "Let's load the mnist dataset and to check if it was loaded correctly we plot the first digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "o_OnKIvHpv9f",
    "outputId": "34dd5609-a82f-4842-fbc5-dddcc5be3d5d"
   },
   "outputs": [],
   "source": [
    "\n",
    "md = MnistDataset()\n",
    "X_train, y_train = md.train_dataset()\n",
    "X_test, y_test = md.test_dataset()\n",
    "\n",
    "first_image = X_train[0,:]\n",
    "first_label = y_train[0]\n",
    "\n",
    "# 784 columns correspond to 28x28 image\n",
    "plottable_image = np.reshape(first_image, (28, 28))\n",
    "# Plot the image\n",
    "plt.imshow(plottable_image, cmap='gray_r')\n",
    "plt.title('Digit Label: {}'.format(first_label))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGW_LPl-pv9g"
   },
   "source": [
    "The following code cell will take time and **memory** to execute but we will save an enourmous ammount of time during the training of the perceptron algorithm.\n",
    "The Gram Matrix store all the kernelized dot product so that the algorithm doesn't need to compute them again.\n",
    "Pay attention to the fact that the polynomial grade has been chosen to compute the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHXfIxlupv9h"
   },
   "outputs": [],
   "source": [
    "# build the gram matrix\n",
    "kernel = 1\n",
    "Gram_train = gram_build(X_train, X_train, kernel)\n",
    "\n",
    "# you need to accept the 'GET MORE RAM'\n",
    "# it will takes a couple of minutes to compute the gram matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcVz007W-7kC"
   },
   "source": [
    "Now we can train the perceptron\n",
    "\n",
    "**epoch**: from 0.1 to 0.9 \n",
    "\n",
    "**kernel**: 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjZ8qRB5pv9i"
   },
   "outputs": [],
   "source": [
    "x1 = np.arange(0.1, 1, 0.1)\n",
    "for i in tqdm(x1):\n",
    "  gram_train_and_store(X_train, y_train, i, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9R2gYhAH_aau"
   },
   "source": [
    "**epoch**: from 1 to 10\n",
    "\n",
    "**kernel**: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKWPv2Papv9k"
   },
   "outputs": [],
   "source": [
    "x1 = np.arange(1,11)\n",
    "for i in tqdm(x1):\n",
    "  gram_train_and_store(X_train, y_train, i, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BB8fYB04AmJi"
   },
   "source": [
    "The last thing now is to remove the matrix from the memory since we need it for the degree = 2 experiment and also because we already stored all the trained perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3mKmUEPAjRv"
   },
   "outputs": [],
   "source": [
    "del Gram_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqN405K1dPmt"
   },
   "source": [
    "To plot the experiment we need to compute the test Gram matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58CodPcndeZB"
   },
   "outputs": [],
   "source": [
    "kernel = 1\n",
    "Gram_test = gram_build(X_test, X_train, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "497mceooAW_2"
   },
   "source": [
    "Finally we can plot the experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "VfxD0fwx4_64",
    "outputId": "ba867521-3810-4f51-a563-d7d478de8b82"
   },
   "outputs": [],
   "source": [
    "error_random = []\n",
    "error_last = []\n",
    "error_avg = []\n",
    "error_vote = []\n",
    "kernel = 1\n",
    "\n",
    "#print(\"epoch: from 0.1 to 0.9 kernel:{}\".format(kernel))\n",
    "x1 = np.arange(0.1, 1, 0.1)\n",
    "x2 = np.arange(1, 11)\n",
    "for i in tqdm(x1):\n",
    "    e_r, e_l, e_a, e_v = gram_load_and_test(X_train, X_test, y_test, i, kernel)\n",
    "    error_random.append(e_r)\n",
    "    error_last.append(e_l)\n",
    "    error_avg.append(e_a)\n",
    "    error_vote.append(e_v)\n",
    "#print(\"epoch: from 1 to 10 kernel:{}\".format(kernel))\n",
    "for i in tqdm(x2):\n",
    "    e_r, e_l, e_a, e_v = gram_load_and_test(X_train, X_test, y_test, i, kernel)\n",
    "    error_random.append(e_r)\n",
    "    error_last.append(e_l)\n",
    "    error_avg.append(e_a)\n",
    "    error_vote.append(e_v)\n",
    "log_plot(np.concatenate((x1, x2)), error_random,\n",
    "          error_last, error_avg, error_vote, kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1mbHZ1ECBs8y"
   },
   "source": [
    "From here we repeat the same phases(training and testing) for the polynomial kernel of degree = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ux_TmRugBIA"
   },
   "outputs": [],
   "source": [
    "del Gram_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbjpYUTdBqW5"
   },
   "outputs": [],
   "source": [
    "kernel = 2\n",
    "Gram_train = gram_build(X_train, X_train, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6OwhT6FCSrA"
   },
   "source": [
    "**epoch**: from 0.1 to 0.9 \n",
    "\n",
    "**kernel**: 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-XlpK8yCLgx"
   },
   "outputs": [],
   "source": [
    "x1 = np.arange(0.1, 1, 0.1)\n",
    "for i in tqdm(x1):\n",
    "  gram_train_and_store(X_train, y_train, i, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Wiul9cjCeTm"
   },
   "source": [
    "With degree = 2 we can train up to 30 epoch (the training and testing phases will be faster)\n",
    "\n",
    "**epoch**: from 1 to 30\n",
    "\n",
    "**kernel**: 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MloPmrNxCPET"
   },
   "outputs": [],
   "source": [
    "x1 = np.arange(1,31)\n",
    "for i in tqdm(x1):\n",
    "  gram_train_and_store(X_train, y_train, i, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_k6GIBtC8Vu"
   },
   "source": [
    "Again we plot the experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAh0GhzPf8vG"
   },
   "outputs": [],
   "source": [
    "del Gram_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Su017dQ2gNlq"
   },
   "outputs": [],
   "source": [
    "kernel = 2\n",
    "Gram_test = gram_build(X_test, X_train, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C2dbuq4O5Ibb"
   },
   "outputs": [],
   "source": [
    "# now for kernel = 2\n",
    "\n",
    "\n",
    "error_random = []\n",
    "error_last = []\n",
    "error_avg = []\n",
    "error_vote = []\n",
    "kernel = 2\n",
    "\n",
    "#print(\"epoch: from 0.1 to 0.9 kernel:{}\".format(kernel))\n",
    "x1 = np.arange(0.1, 1, 0.1)\n",
    "x2 = np.arange(1, 31)\n",
    "for i in tqdm(x1):\n",
    "    e_r, e_l, e_a, e_v = load_and_test(X_train, X_test, y_test, i, kernel)\n",
    "    error_random.append(e_r)\n",
    "    error_last.append(e_l)\n",
    "    error_avg.append(e_a)\n",
    "    error_vote.append(e_v)\n",
    "#print(\"epoch: from 1 to 30 kernel:{}\".format(kernel))\n",
    "for i in tqdm(x2):\n",
    "    e_r, e_l, e_a, e_v = load_and_test(X_train, X_test, y_test, i, kernel)\n",
    "    error_random.append(e_r)\n",
    "    error_last.append(e_l)\n",
    "    error_avg.append(e_a)\n",
    "    error_vote.append(e_v)\n",
    "log_plot(np.concatenate((x1, x2)), error_random,\n",
    "          error_last, error_avg, error_vote, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1r0Vs_9q9dnQ"
   },
   "source": [
    "We can also download the pretrained models to store them eventually.\n",
    "\n",
    "First we zip the models folder and then we download the ziped folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qU75Bt2F9ctT"
   },
   "outputs": [],
   "source": [
    "!zip -r ./models.zip ./models\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download('models.zip')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "voted-perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
